%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%    Sharp interface limits    %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@misc{Mellet.Rozowski_2024_VolumepreservingMeancurvatureFlow,
	title = {Volume-Preserving Mean-Curvature Flow as a Singular Limit of a Diffusion-Aggregation Equation},
	author = {Mellet, Antoine and Rozowski, Michael},
	year = {2024},
	eprint = {2408.14309},
	primaryclass = {math.AP},
	publisher = {arXiv},
	doi = {10.48550/ARXIV.2408.14309},
	archiveprefix = {arXiv},
	arxiv = {2408.14309},
	abstract = {The Patlak-Keller-Segel system of equations (PKS) is a classical example of aggregation-diffusion equation in which the repulsive effect of diffusion is in competition with the attractive chemotaxis term. 
	Recent work on the <em>Parabolic-Elliptic PKS model</em> have shown that when the repulsion is modeled by a nonlinear diffusion term \(\rho \nabla \rho^{m-1}\) with \(m>2\), this competition leads to phase separation phenomena. Furthermore, in some asymptotic regime corresponding to a large population observed over a long enough time, the interface separating regions of high and low density evolves according to the Hele-Shaw free boundary problem with surface tension. 
	In the present paper, we consider the counterpart of that model, namely the  <em>Elliptic-Parabolic PKS model</em> and we prove that the same phase separation phenomena occurs, but the motion of the interface is now described (asymptotically) by a volume-preserving mean-curvature flow.
	},
	keywords = {Allen-Cahn,Analysis of PDEs (math.AP),BV solutions,chemotaxis,conditional convergence,diffuse interface approximation,existence of solutions,FOS: Mathematics,Mathematics - Analysis of PDEs,minimizing movement scheme,nonlinear diffusion,sharp interface limit,uniqueness of solutions,volume-preserving mean-curvature flow}
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%    Neural networks    %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{Rozowski.etal_2022_Inputlayerregularization,
	author = {Rozowski, Michael and Palumbo, Jonathan and Bisen, Jay and Bi, Chuan and Bouhrara, Mustapha and Czaja, Wojciech and Spencer, Richard G.},
	title = {Input layer regularization for magnetic resonance relaxometry biexponential parameter estimation},
	journal = {Magnetic Resonance in Chemistry},
	volume = {60},
	number = {11},
	pages = {1076-1086},
	keywords = {biexponentials, deep learning, MRI, neural network, parameter estimation, regularization, relaxometry},
	doi = {10.1002/mrc.5289},
	url = {https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/mrc.5289},
	eprint = {https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/pdf/10.1002/mrc.5289},
	abstract = {Many methods have been developed for estimating the parameters of biexponential decay signals, which arise throughout magnetic resonance relaxometry (MRR) and the physical sciences. This is an intrinsically ill-posed problem so that estimates can depend strongly on noise and underlying parameter values. Regularization has proven to be a remarkably efficient procedure for providing more reliable solutions to ill-posed problems, while, more recently, neural networks have been used for parameter estimation. We re-address the problem of parameter estimation in biexponential models by introducing a novel form of neural network regularization which we call input layer regularization (ILR). Here, inputs to the neural network are composed of a biexponential decay signal augmented by signals constructed from parameters obtained from a regularized nonlinear least-squares estimate of the two decay time constants. We find that ILR results in a reduction in the error of time constant estimates on the order of 15%â€“50% or more, depending on the metric used and signal-to-noise level, with greater improvement seen for the time constant of the more rapidly decaying component. ILR is compatible with existing regularization techniques and should be applicable to a wide range of parameter estimation problems.},
	year = {2022}
}

